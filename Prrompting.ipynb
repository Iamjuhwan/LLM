{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyO8hPkf3HDcl30bWasnCYEX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Iamjuhwan/LLM/blob/main/Prrompting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_3werRfH-Iwu",
        "outputId": "52c536a7-05e9-4e80-9990-0b021d8fd2f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping jupyterlab as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip uninstall -qqy jupyterlab  # Remove unused packages from Kaggle's base image that conflict\n",
        "!pip install -U -q \"google-genai==1.7.0\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google import genai\n",
        "from google.genai import types\n",
        "\n",
        "from IPython.display import HTML, Markdown, display"
      ],
      "metadata": {
        "id": "UA-dEFXR-NCJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.api_core import retry\n",
        "\n",
        "\n",
        "is_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503})\n",
        "\n",
        "genai.models.Models.generate_content = retry.Retry(\n",
        "    predicate=is_retriable)(genai.models.Models.generate_content)"
      ],
      "metadata": {
        "id": "tPKA1r8I-b88"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "\n",
        "GOOGLE_API_KEY = \"AIzaSyC4CnYxMTMFCFHS09D_kDj-oqQ7X9U8LZQ\"\n",
        "\n",
        "# Option 2: Load from environment variable (more secure)\n",
        "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
        "\n"
      ],
      "metadata": {
        "id": "sxCvbZrh-ofF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client = genai.Client(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    contents=\"Explain AI to me like I'm a novice.\")\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8JY79P-e-3Vc",
        "outputId": "c527746a-4a9a-4db5-d8e2-95852a7cc6a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Okay, imagine you have a really smart puppy.\n",
            "\n",
            "**Regular puppy:** You teach it \"sit\" by showing it, rewarding it, and repeating. It learns to associate the word \"sit\" with the action and the treat. It can only \"sit\" on command.\n",
            "\n",
            "**AI puppy:** This puppy is different. Instead of just being told what to do, you feed it a **ton** of examples. You show it thousands of pictures of dogs sitting, maybe some videos too.  You tell it, \"These are examples of 'sit'.\"\n",
            "\n",
            "After seeing all these examples, the AI puppy starts to **recognize patterns** on its own. It learns what sitting generally *looks like*, even if the dog is a different breed, a different color, or sitting on a different surface.\n",
            "\n",
            "Now, when you show it a *new* picture of a dog sitting that it's NEVER seen before, it can **predict** with pretty good accuracy, \"Hey! That looks like 'sit'!\"\n",
            "\n",
            "That, in its simplest form, is AI.  Here's a breakdown:\n",
            "\n",
            "*   **AI stands for Artificial Intelligence.** It's about creating machines that can perform tasks that typically require human intelligence.\n",
            "\n",
            "*   **The key is LEARNING.** AI doesn't just follow a rigid set of instructions. It learns from data.  The more data it has, the better it becomes.\n",
            "\n",
            "*   **It works with PATTERNS.** AI looks for patterns in the data it's given.\n",
            "\n",
            "*   **It makes PREDICTIONS or DECISIONS.** Based on those patterns, AI can predict what might happen next or make a decision about the best course of action.\n",
            "\n",
            "**Think of it like this:**\n",
            "\n",
            "*   **Human Intelligence:** You see a million different chairs and you know they are all chairs.  You can recognize a new, weirdly shaped chair you've never seen before.\n",
            "*   **Artificial Intelligence:** The AI learns from seeing images of a million different chairs.  It then can predict with high probability that a new image is also a chair.\n",
            "\n",
            "**Examples you see every day:**\n",
            "\n",
            "*   **Spam filters in your email:** The AI learns to recognize what spam looks like based on millions of spam emails.\n",
            "*   **Movie recommendations on Netflix:** The AI learns your viewing habits and suggests movies you might like.\n",
            "*   **Voice assistants like Siri or Alexa:** They learn to understand your voice and commands.\n",
            "*   **Self-driving cars:** They learn to recognize traffic signals, pedestrians, and other vehicles.\n",
            "\n",
            "**Important things to remember:**\n",
            "\n",
            "*   **AI is a tool.** It's designed to help us solve problems and automate tasks.\n",
            "*   **AI isn't magic.** It's based on algorithms and data.\n",
            "*   **AI is constantly evolving.** It's a rapidly developing field with new advancements all the time.\n",
            "*   **AI can be biased.** If the data it learns from is biased, the AI will also be biased.\n",
            "\n",
            "So, to recap, AI is about giving computers the ability to learn from data, recognize patterns, and make predictions or decisions, much like a really, really smart puppy that learns by example.  Hopefully, this helps you understand the basics! Let me know if you have any more questions.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(response.text)"
      ],
      "metadata": {
        "id": "p7JwrtfQAD-J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "0_PTbfCU_p8_",
        "outputId": "e3f76e72-d8e3-4e72-9593-5b2dceb58340"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Okay, imagine you have a really smart puppy.\\n\\n**Regular puppy:** You teach it \"sit\" by showing it, rewarding it, and repeating. It learns to associate the word \"sit\" with the action and the treat. It can only \"sit\" on command.\\n\\n**AI puppy:** This puppy is different. Instead of just being told what to do, you feed it a **ton** of examples. You show it thousands of pictures of dogs sitting, maybe some videos too.  You tell it, \"These are examples of \\'sit\\'.\"\\n\\nAfter seeing all these examples, the AI puppy starts to **recognize patterns** on its own. It learns what sitting generally *looks like*, even if the dog is a different breed, a different color, or sitting on a different surface.\\n\\nNow, when you show it a *new* picture of a dog sitting that it\\'s NEVER seen before, it can **predict** with pretty good accuracy, \"Hey! That looks like \\'sit\\'!\"\\n\\nThat, in its simplest form, is AI.  Here\\'s a breakdown:\\n\\n*   **AI stands for Artificial Intelligence.** It\\'s about creating machines that can perform tasks that typically require human intelligence.\\n\\n*   **The key is LEARNING.** AI doesn\\'t just follow a rigid set of instructions. It learns from data.  The more data it has, the better it becomes.\\n\\n*   **It works with PATTERNS.** AI looks for patterns in the data it\\'s given.\\n\\n*   **It makes PREDICTIONS or DECISIONS.** Based on those patterns, AI can predict what might happen next or make a decision about the best course of action.\\n\\n**Think of it like this:**\\n\\n*   **Human Intelligence:** You see a million different chairs and you know they are all chairs.  You can recognize a new, weirdly shaped chair you\\'ve never seen before.\\n*   **Artificial Intelligence:** The AI learns from seeing images of a million different chairs.  It then can predict with high probability that a new image is also a chair.\\n\\n**Examples you see every day:**\\n\\n*   **Spam filters in your email:** The AI learns to recognize what spam looks like based on millions of spam emails.\\n*   **Movie recommendations on Netflix:** The AI learns your viewing habits and suggests movies you might like.\\n*   **Voice assistants like Siri or Alexa:** They learn to understand your voice and commands.\\n*   **Self-driving cars:** They learn to recognize traffic signals, pedestrians, and other vehicles.\\n\\n**Important things to remember:**\\n\\n*   **AI is a tool.** It\\'s designed to help us solve problems and automate tasks.\\n*   **AI isn\\'t magic.** It\\'s based on algorithms and data.\\n*   **AI is constantly evolving.** It\\'s a rapidly developing field with new advancements all the time.\\n*   **AI can be biased.** If the data it learns from is biased, the AI will also be biased.\\n\\nSo, to recap, AI is about giving computers the ability to learn from data, recognize patterns, and make predictions or decisions, much like a really, really smart puppy that learns by example.  Hopefully, this helps you understand the basics! Let me know if you have any more questions.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RzvjLZmQAGYz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ep-zrH8uANZo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat = client.chats.create(model='gemini-2.0-flash', history=[])\n",
        "response = chat.send_message('Hello! My name is Jesujuwon.')\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HavKY5FUAN4o",
        "outputId": "6e2cd321-7ad5-48f4-ad3e-b73da3161ee5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello Jesujuwon! It's nice to meet you. How can I help you today?\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = chat.send_message('Can you tell me something interesting about writing CFA exam?')\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QMvs-BHCARIQ",
        "outputId": "e703f6e4-2382-45c1-dfba-b707893aab46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Alright, Jesujuwon, here's an interesting fact about the CFA exam:\n",
            "\n",
            "**Despite being known for its rigorous curriculum and low pass rates, the CFA Program was initially conceived as a way to standardize and professionalize investment analysis at a time when the field was considered rather unregulated and even prone to dubious practices.**\n",
            "\n",
            "Think about it: back in the mid-20th century, the world of investment management was quite different. Information wasn't readily available, ethical standards varied, and the profession lacked a clear, recognized benchmark of competence.\n",
            "\n",
            "**Here's why this is interesting:**\n",
            "\n",
            "*   **Humble Beginnings:** The CFA Program wasn't always the global standard it is today. It started as a grassroots effort to establish a common body of knowledge and ethical guidelines in a somewhat Wild West environment.\n",
            "*   **Focus on Ethics:** A core element of the program from its inception was (and remains) ethical conduct. This was a direct response to the need for greater integrity in the investment profession. The emphasis on ethics is not just a token gesture, but a fundamental aspect that permeates the entire curriculum and examination.\n",
            "*   **Impact on the Industry:** The CFA Program has played a significant role in shaping the investment management industry, driving higher standards of professionalism, competence, and ethical behavior globally. Its influence extends far beyond just the individuals who earn the charter.\n",
            "*   **Evolution:** The CFA Program has constantly evolved to keep pace with the changing financial landscape, incorporating new topics, technologies, and regulatory developments into the curriculum.\n",
            "\n",
            "So, the next time you think about the daunting task of preparing for the CFA exam, remember that it represents a significant effort to elevate the investment profession and promote ethical and informed decision-making. It's not just about passing a test; it's about contributing to a more responsible and reliable financial world.\n",
            "\n",
            "Does that pique your interest, Jesujuwon?\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = chat.send_message('Do you remember what my name is?')\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7CUpw1-IAY_P",
        "outputId": "37af40fd-64a2-463b-e1b7-2e268fe1d45d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Yes, Jesujuwon. I remember your name is Jesujuwon.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zNeKx8UVBKtI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Choosing a Model"
      ],
      "metadata": {
        "id": "bhxLz2kCBQKW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for model in client.models.list():\n",
        "  print(model.name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wSDHmqqYBSgf",
        "outputId": "b42828a5-0706-4729-8a62-4d6a10fb736b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "models/chat-bison-001\n",
            "models/text-bison-001\n",
            "models/embedding-gecko-001\n",
            "models/gemini-1.0-pro-vision-latest\n",
            "models/gemini-pro-vision\n",
            "models/gemini-1.5-pro-latest\n",
            "models/gemini-1.5-pro-001\n",
            "models/gemini-1.5-pro-002\n",
            "models/gemini-1.5-pro\n",
            "models/gemini-1.5-flash-latest\n",
            "models/gemini-1.5-flash-001\n",
            "models/gemini-1.5-flash-001-tuning\n",
            "models/gemini-1.5-flash\n",
            "models/gemini-1.5-flash-002\n",
            "models/gemini-1.5-flash-8b\n",
            "models/gemini-1.5-flash-8b-001\n",
            "models/gemini-1.5-flash-8b-latest\n",
            "models/gemini-1.5-flash-8b-exp-0827\n",
            "models/gemini-1.5-flash-8b-exp-0924\n",
            "models/gemini-2.5-pro-exp-03-25\n",
            "models/gemini-2.5-pro-preview-03-25\n",
            "models/gemini-2.0-flash-exp\n",
            "models/gemini-2.0-flash\n",
            "models/gemini-2.0-flash-001\n",
            "models/gemini-2.0-flash-exp-image-generation\n",
            "models/gemini-2.0-flash-lite-001\n",
            "models/gemini-2.0-flash-lite\n",
            "models/gemini-2.0-flash-lite-preview-02-05\n",
            "models/gemini-2.0-flash-lite-preview\n",
            "models/gemini-2.0-pro-exp\n",
            "models/gemini-2.0-pro-exp-02-05\n",
            "models/gemini-exp-1206\n",
            "models/gemini-2.0-flash-thinking-exp-01-21\n",
            "models/gemini-2.0-flash-thinking-exp\n",
            "models/gemini-2.0-flash-thinking-exp-1219\n",
            "models/learnlm-1.5-pro-experimental\n",
            "models/gemma-3-1b-it\n",
            "models/gemma-3-4b-it\n",
            "models/gemma-3-12b-it\n",
            "models/gemma-3-27b-it\n",
            "models/embedding-001\n",
            "models/text-embedding-004\n",
            "models/gemini-embedding-exp-03-07\n",
            "models/gemini-embedding-exp\n",
            "models/aqa\n",
            "models/imagen-3.0-generate-002\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint\n",
        "\n",
        "for model in client.models.list():\n",
        "  if model.name == 'models/gemini-2.0-flash':\n",
        "    pprint(model.to_json_dict())\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "351OUCRkBTLj",
        "outputId": "82914b38-fd46-4925-b2fe-177033d7d982"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'description': 'Gemini 2.0 Flash',\n",
            " 'display_name': 'Gemini 2.0 Flash',\n",
            " 'input_token_limit': 1048576,\n",
            " 'name': 'models/gemini-2.0-flash',\n",
            " 'output_token_limit': 8192,\n",
            " 'supported_actions': ['generateContent', 'countTokens'],\n",
            " 'tuned_model_info': {},\n",
            " 'version': '2.0'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Yf1OdzHZBaEJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exploring generation parameters"
      ],
      "metadata": {
        "id": "4fcyerkmBiVa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.genai import types\n",
        "\n",
        "short_config = types.GenerateContentConfig(max_output_tokens=200)\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model='gemini-2.0-flash',\n",
        "    config=short_config,\n",
        "    contents='Write a 2000 word essay for my girlfriend Abigeal and tell her how beautiful and amazing she is.')\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ebsjlo6pBlFC",
        "outputId": "78a2c7a7-a1d6-4c46-a822-d0063caf990d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Okay, here is a 2000-word essay dedicated to Abigeal, focusing on her beauty, inner strength, and the profound impact she has on your life.  I've tried to make it as genuine and heartfelt as possible.  Remember to personalize it further with specific memories, inside jokes, and details that are unique to your relationship.  Good luck!\n",
            "\n",
            "***\n",
            "\n",
            "**An Ode to Abigeal: A Tapestry of Beauty, Grace, and Unending Wonder**\n",
            "\n",
            "My dearest Abigeal,\n",
            "\n",
            "Words, those frail vessels of expression, often feel inadequate when I attempt to capture the essence of you. How can I possibly translate the symphony of emotions you evoke within me, the kaleidoscope of colors you paint upon my world, into something tangible, something that truly reflects the profound impact you have on my life?  This essay, then, is not merely a collection of sentences; it is a heartfelt attempt to articulate the beauty I see in you,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = client.models.generate_content(\n",
        "    model='gemini-2.0-flash',\n",
        "    config=short_config,\n",
        "    contents='Write a short poem on the love of God.')\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Ukci4QqBo72",
        "outputId": "31bcb581-a509-4d08-89be-dfff98fd96c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A love that knows no bounds, no end,\n",
            "A gentle hand, a faithful friend.\n",
            "Through valleys dark and mountains high,\n",
            "His light will shine, across the sky.\n",
            "\n",
            "A whisper soft, a mighty roar,\n",
            "He knocks upon the heart's closed door.\n",
            "A sacrifice, a crimson stain,\n",
            "To wash away all sin and pain.\n",
            "\n",
            "So trust His grace, and feel His peace,\n",
            "A love that will forever increase.\n",
            "For in His arms, you'll truly see,\n",
            "The boundless love eternally.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0ELMmi6KCUqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Temperature"
      ],
      "metadata": {
        "id": "dUPxASHgCe51"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "high_temp_config = types.GenerateContentConfig(temperature=2.0)\n",
        "\n",
        "\n",
        "for _ in range(5):\n",
        "  response = client.models.generate_content(\n",
        "      model='gemini-2.0-flash',\n",
        "      config=high_temp_config,\n",
        "      contents='Pick a random colour... (respond in a single word)')\n",
        "\n",
        "  if response.text:\n",
        "    print(response.text, '-' * 25)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JhAoTapICg2l",
        "outputId": "69f450fd-54c8-4e17-a175-1a672812c987"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Orange\n",
            " -------------------------\n",
            "Azure\n",
            " -------------------------\n",
            "Azure.\n",
            " -------------------------\n",
            "Magenta\n",
            " -------------------------\n",
            "Orange\n",
            " -------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Response with a very low temp"
      ],
      "metadata": {
        "id": "gzO_2hXzCocb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "low_temp_config = types.GenerateContentConfig(temperature=0.0)\n",
        "\n",
        "for _ in range(5):\n",
        "  response = client.models.generate_content(\n",
        "      model='gemini-2.0-flash',\n",
        "      config=low_temp_config,\n",
        "      contents='Pick a random colour... (respond in a single word)')\n",
        "\n",
        "  if response.text:\n",
        "    print(response.text, '-' * 25)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rM8UEfG2ChmA",
        "outputId": "c6e5a9c7-ce8d-45af-eee5-988820b0cb6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Azure\n",
            " -------------------------\n",
            "Azure\n",
            " -------------------------\n",
            "Azure\n",
            " -------------------------\n",
            "Azure\n",
            " -------------------------\n",
            "Azure\n",
            " -------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Top-P"
      ],
      "metadata": {
        "id": "T5sCVQolC1TZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Like temperature, the top-P parameter is also used to control the diversity of the model's output.\n",
        "\n",
        "Top-P defines the probability threshold that, once cumulatively exceeded, tokens stop being selected as candidates. A top-P of 0 is typically equivalent to greedy decoding, and a top-P of 1 typically selects every token in the model's vocabulary.\n",
        "\n",
        "Top-K is not configurable in the Gemini 2.0 series of models, but can be changed in older models. Top-K is a positive integer that defines the number of most probable tokens from which to select the output token. A top-K of 1 selects a single token, performing greedy decoding.\n"
      ],
      "metadata": {
        "id": "82IgdP1jC4jI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_config = types.GenerateContentConfig(\n",
        "    # These are the default values for gemini-2.0-flash.\n",
        "    temperature=1.0,\n",
        "    top_p=0.95,\n",
        ")\n",
        "\n",
        "story_prompt = \"You are a creative writer. Write a short story about a tech bro facing issue trying to debug.\"\n",
        "response = client.models.generate_content(\n",
        "    model='gemini-2.0-flash',\n",
        "    config=model_config,\n",
        "    contents=story_prompt)\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y1pwRAxAC0je",
        "outputId": "59c5c67c-f4d0-47fd-eb84-40272e711c85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The hum of the server room was a lullaby to Chad. He practically lived here. Cocooned in the cool, sterile air, fueled by Soylent and the righteous fire of innovation, he was building the next big thing: \"MemeLord,\" a decentralized platform for AI-generated memes. Think TikTok meets blockchain meets… well, Chad's genius.\n",
            "\n",
            "Tonight, though, the lullaby was a discordant drone. MemeLord, scheduled for its beta launch in 48 hours, was spitting out gibberish. Not the good kind of gibberish, the surreal, dadaist AI-generated art that made the investors drool. This was raw, unadulterated garbage. ASCII vomit.\n",
            "\n",
            "Chad, clad in his signature black turtleneck and the haunted gaze of a man who hadn't slept properly in a week, hammered at his keyboard. He’d refactored the GAN model three times, tweaked the reward function, even tried sacrificing a vintage floppy disk to the gods of debugging (a waste, he'd later realized, as he couldn't find the ritual knife).\n",
            "\n",
            "“Stupid thing,” he muttered, running a hand through his already disheveled, perfectly sculpted hair. He slammed his fist on the table, rattling his collection of fidget spinners.\n",
            "\n",
            "He'd built this thing. He understood every line of code, every intricate algorithm. Except, apparently, this one glaring, pulsating flaw.\n",
            "\n",
            "He tried the standard debugging arsenal. Print statements littered the code like confetti after a particularly sad parade. He stepped through the code line by line, the debugger blinking back at him mockingly. He even, in a moment of desperation, consulted Stack Overflow, only to be met with a wall of cryptic answers and condescending comments about his coding style.\n",
            "\n",
            "\"Are you SURE you're using the right libraries, bro?\" one comment sneered.\n",
            "\n",
            "Chad felt a vein throb in his forehead. Libraries? He practically *invented* libraries! He was a coding god!\n",
            "\n",
            "He stalked over to the whiteboard, covered in a chaotic tapestry of diagrams and half-formed ideas. He stared at it, willing the solution to materialize. He saw the architecture, the flow, the… wait. Something was nagging at him. A faint tickle in the back of his brain.\n",
            "\n",
            "He squinted. In the bottom corner, almost obscured by a dried-up marker stain, was a tiny, barely legible note he'd scribbled days ago.\n",
            "\n",
            "\"REMEMBER TO UNCOMMENT THE RANDOM SEED!!!\"\n",
            "\n",
            "Chad froze. His stomach dropped. The blood drained from his face.\n",
            "\n",
            "The random seed. He'd been so focused on the grand, complex architecture that he'd forgotten the simplest, most fundamental element of the entire system. The random seed, the crucial bit of code that injected unpredictability into the AI's creative process, was still commented out. The AI wasn't generating memes, it was regurgitating the same deterministic output over and over again, a horrifying, pixelated echo of its own blank slate.\n",
            "\n",
            "He scrambled back to his keyboard, his fingers trembling. He located the offending line, his heart pounding. With a shaky breath, he deleted the \"//\".\n",
            "\n",
            "He ran the program.\n",
            "\n",
            "The server hummed, and on the screen, a meme began to coalesce. It was bizarre, absurd, and faintly disturbing: a picture of a Shiba Inu wearing a monocle, superimposed over a stock photo of a broccoli floret, with the caption: \"Existential Dread: Now Available in Microgreens.\"\n",
            "\n",
            "Chad stared at it, then let out a shaky laugh that echoed through the server room. It wasn't perfect. It wasn't even particularly funny. But it was something. It was chaotic, unpredictable, and uniquely MemeLord.\n",
            "\n",
            "He leaned back in his chair, the tension draining from his body. He still had 48 hours, a mountain of work, and a looming beta launch. But for now, he had a seed. And sometimes, that's all you need to grow something truly… meme-orable. He even allowed himself a small, exhausted smile. He was, after all, still a coding god. He just needed a little reminder now and then. And maybe, just maybe, a decent night's sleep.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zKbZb6WXCnaY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Zero-shot"
      ],
      "metadata": {
        "id": "YN5oan8xDeq2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Zero-shot prompts are prompts that describe the request for the model directly."
      ],
      "metadata": {
        "id": "EYgg2m0RDhuV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_config = types.GenerateContentConfig(\n",
        "    temperature=0.1,\n",
        "    top_p=1,\n",
        "    max_output_tokens=5,\n",
        ")\n",
        "\n",
        "zero_shot_prompt = \"\"\"Classify movie reviews as POSITIVE, NEUTRAL or NEGATIVE.\n",
        "Review: \"Her\" is a disturbing study revealing the direction\n",
        "humanity is headed if AI is allowed to keep evolving,\n",
        "unchecked. I wish there were more movies like this masterpiece.\n",
        "Sentiment: \"\"\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model='gemini-2.0-flash',\n",
        "    config=model_config,\n",
        "    contents=zero_shot_prompt)\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5MkgE8zxDhKD",
        "outputId": "cbf32350-014b-436a-e36a-5f01ee47ae09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "POSITIVE\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4nz-mndODoDB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Enum Model"
      ],
      "metadata": {
        "id": "pwxpAtNNDzWv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The models are trained to generate text, and while the Gemini 2.0 models are great at following instructions, other models can sometimes produce more text than you may wish for. In the preceding example, the model will output the label, but sometimes it can include a preceding \"Sentiment\" label, and without an output token limit, it may also add explanatory text afterwards"
      ],
      "metadata": {
        "id": "PGwfn0O9D19t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import enum\n",
        "\n",
        "class Sentiment(enum.Enum):\n",
        "    POSITIVE = \"positive\"\n",
        "    NEUTRAL = \"neutral\"\n",
        "    NEGATIVE = \"negative\"\n",
        "\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model='gemini-2.0-flash',\n",
        "    config=types.GenerateContentConfig(\n",
        "        response_mime_type=\"text/x.enum\",\n",
        "        response_schema=Sentiment\n",
        "    ),\n",
        "    contents=zero_shot_prompt)\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oykwYxm9D1Wt",
        "outputId": "d091a191-db39-477a-b740-095f0d8f2bdf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "enum_response = response.parsed\n",
        "print(enum_response)\n",
        "print(type(enum_response))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zlNluLOGD7aJ",
        "outputId": "7dd09834-097b-4750-f39d-a16c5bb816a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment.POSITIVE\n",
            "<enum 'Sentiment'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lL18ZWX-ECUA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## One-shot and few-shot"
      ],
      "metadata": {
        "id": "bgOqFHkSFAEN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Mm_HYEP4FCpm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "few_shot_prompt = \"\"\"Parse a customer's pizza order into valid JSON:\n",
        "\n",
        "EXAMPLE:\n",
        "I want a small pizza with cheese, tomato sauce, and pepperoni.\n",
        "JSON Response:\n",
        "```\n",
        "{\n",
        "\"size\": \"small\",\n",
        "\"type\": \"normal\",\n",
        "\"ingredients\": [\"cheese\", \"tomato sauce\", \"pepperoni\"]\n",
        "}\n",
        "```\n",
        "\n",
        "EXAMPLE:\n",
        "Can I get a large pizza with tomato sauce, basil and mozzarella\n",
        "JSON Response:\n",
        "```\n",
        "{\n",
        "\"size\": \"large\",\n",
        "\"type\": \"normal\",\n",
        "\"ingredients\": [\"tomato sauce\", \"basil\", \"mozzarella\"]\n",
        "}\n",
        "```\n",
        "\n",
        "ORDER:\n",
        "\"\"\"\n",
        "\n",
        "customer_order = \"Give me a large with cheese & pineapple\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model='gemini-2.0-flash',\n",
        "    config=types.GenerateContentConfig(\n",
        "        temperature=0.1,\n",
        "        top_p=1,\n",
        "        max_output_tokens=250,\n",
        "    ),\n",
        "    contents=[few_shot_prompt, customer_order])\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qWMvl9whFByn",
        "outputId": "ee42b124-e7a3-4bbd-bb1d-702e6332e2b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "```json\n",
            "{\n",
            "\"size\": \"large\",\n",
            "\"type\": \"normal\",\n",
            "\"ingredients\": [\"cheese\", \"pineapple\"]\n",
            "}\n",
            "```\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import typing_extensions as typing\n",
        "\n",
        "class PizzaOrder(typing.TypedDict):\n",
        "    size: str\n",
        "    ingredients: list[str]\n",
        "    type: str\n",
        "\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model='gemini-2.0-flash',\n",
        "    config=types.GenerateContentConfig(\n",
        "        temperature=0.1,\n",
        "        response_mime_type=\"application/json\",\n",
        "        response_schema=PizzaOrder,\n",
        "    ),\n",
        "    contents=\"Can I have a large dessert pizza with apple and chocolate\")\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O-exU5MLFL_G",
        "outputId": "f542aa47-c4a3-49a9-8243-e7720d8f12ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"size\": \"large\",\n",
            "  \"ingredients\": [\"apple\", \"chocolate\"],\n",
            "  \"type\": \"dessert\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JVZpr6uYFS2U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chain of Thought (CoT)"
      ],
      "metadata": {
        "id": "iNJbI9R_Ff9O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Direct prompting on LLMs can return answers quickly and (in terms of output token usage) efficiently, but they can be prone to hallucination. The answer may \"look\" correct (in terms of language and syntax) but is incorrect in terms of factuality and reasoning.\n",
        "\n",
        "Chain-of-Thought prompting is a technique where you instruct the model to output intermediate reasoning steps, and it typically gets better results, especially when combined with few-shot examples. It is worth noting that this technique doesn't completely eliminate hallucinations, and that it tends to cost more to run, due to the increased token count.\n",
        "\n",
        "Models like the Gemini family are trained to be \"chatty\" or \"thoughtful\" and will provide reasoning steps without prompting, so for this simple example you can ask the model to be more direct in the prompt to force a non-reasoning response."
      ],
      "metadata": {
        "id": "YFMNlBZnFemf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"When I was 4 years old, my partner was 3 times my age. Now, I\n",
        "am 20 years old. How old is my partner? Return the answer directly.\"\"\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model='gemini-2.0-flash',\n",
        "    contents=prompt)\n",
        "\n",
        "print(response.text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5IwFuffuFiD3",
        "outputId": "c0219f86-9a42-46f4-b7f7-d1c4c860f7b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "52\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"When I was 4 years old, my partner was 3 times my age. Now,\n",
        "I am 20 years old. How old is my partner? Let's think step by step.\"\"\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model='gemini-2.0-flash',\n",
        "    contents=prompt)\n",
        "\n",
        "Markdown(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "id": "4lkeQyskFom_",
        "outputId": "93bc8369-3ecb-420a-d831-f0ec3184a617"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Here's how to solve the problem step-by-step:\n\n1.  **Find the age difference:** When you were 4, your partner was 3 times your age, meaning they were 4 * 3 = 12 years old.\n2.  **Calculate the age difference:** The age difference between you and your partner is 12 - 4 = 8 years.\n3.  **Determine the partner's current age:** Since you are now 20 and the age difference remains the same, your partner is 20 + 8 = 28 years old.\n\n**Therefore, your partner is currently 28 years old.**"
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_instructions = \"\"\"You are an AI assistant that uses a ReAct prompting strategy.\n",
        "Think step by step and reason through the question before answering.\"\"\"\n",
        "\n",
        "example1 = \"\"\"\n",
        "Question: What is the capital of France?\n",
        "Thought: I know that France is a country in Europe. Its capital is a major global city.\n",
        "Answer: Paris\n",
        "\"\"\"\n",
        "\n",
        "example2 = \"\"\"\n",
        "Question: Who was the youngest author listed on the transformers NLP paper?\n",
        "\"\"\"\n",
        "\n",
        "react_config = types.GenerateContentConfig(\n",
        "    stop_sequences=[\"\\nObservation\"],\n",
        "    system_instruction=model_instructions + example1 + example2,\n",
        ")\n",
        "\n",
        "# Create a chat that has the model instructions and examples pre-seeded.\n",
        "react_chat = client.chats.create(\n",
        "    model='gemini-2.0-flash',\n",
        "    config=react_config,\n",
        ")\n",
        "\n",
        "resp = react_chat.send_message(question)\n",
        "print(resp.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4IgzyViVGXtx",
        "outputId": "b91a842e-7f7b-48a9-8b3d-9d292ff4b6d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Okay, I need to figure out who the youngest author of the \"Attention is All You Need\" paper (the Transformers paper) is. Here's my plan:\n",
            "\n",
            "1. **Identify the authors of the paper:**  I'll start by finding the \"Attention is All You Need\" paper and listing the authors.\n",
            "2. **Find the birthdates of the authors:**  I'll then search for the birthdates of each of the authors.\n",
            "3. **Calculate the ages of the authors at the time of publication:** Since the paper was published in 2017, I will calculate the ages of the authors in 2017.\n",
            "4. **Determine the youngest author:** Finally, I'll compare the ages and identify the youngest author.\n",
            "\n",
            "Let's start!\n",
            "\n",
            "**Step 1: Identify the authors of the paper**\n",
            "\n",
            "The authors of the \"Attention is All You Need\" paper are:\n",
            "\n",
            "*   Ashish Vaswani\n",
            "*   Noam Shazeer\n",
            "*   Niki Parmar\n",
            "*   Jakob Uszkoreit\n",
            "*   Llion Jones\n",
            "*   Aidan N. Gomez\n",
            "*   Łukasz Kaiser\n",
            "*   Illia Polosukhin\n",
            "\n",
            "**Step 2: Find the birthdates of the authors**\n",
            "\n",
            "Now I will search for the birthdates of each of these authors:\n",
            "\n",
            "*   Ashish Vaswani: I couldn't find a reliable source of his birthdate.\n",
            "*   Noam Shazeer: I couldn't find a reliable source of his birthdate.\n",
            "*   Niki Parmar: I couldn't find a reliable source of her birthdate.\n",
            "*   Jakob Uszkoreit: I couldn't find a reliable source of his birthdate.\n",
            "*   Llion Jones: I couldn't find a reliable source of his birthdate.\n",
            "*   Aidan N. Gomez: November 26, 1994\n",
            "*   Łukasz Kaiser: I couldn't find a reliable source of his birthdate.\n",
            "*   Illia Polosukhin: I couldn't find a reliable source of his birthdate.\n",
            "\n",
            "**Step 3: Calculate the ages of the authors at the time of publication**\n",
            "\n",
            "Since the paper was published in 2017, I can calculate Aidan N. Gomez's age:\n",
            "\n",
            "*   Aidan N. Gomez: Born November 26, 1994. In 2017, he was 22 years old.\n",
            "\n",
            "**Step 4: Determine the youngest author**\n",
            "\n",
            "Since I could only find the birthdate of Aidan N. Gomez, and none of the other authors' ages are confirmed to be younger than 22 in 2017, I will assume that he is the youngest.\n",
            "\n",
            "**Answer:**\n",
            "\n",
            "Aidan N. Gomez was the youngest author listed on the Transformers NLP paper.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "from IPython.display import Markdown, clear_output\n",
        "\n",
        "\n",
        "response = client.models.generate_content_stream(\n",
        "    model='gemini-2.0-flash-thinking-exp',\n",
        "    contents='Who was the youngest author listed on the transformers NLP paper?',\n",
        ")\n",
        "\n",
        "buf = io.StringIO()\n",
        "for chunk in response:\n",
        "    buf.write(chunk.text)\n",
        "    # Display the response as it is streamed\n",
        "    print(chunk.text, end='')\n",
        "\n",
        "# And then render the finished response as formatted markdown.\n",
        "clear_output()\n",
        "Markdown(buf.getvalue())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 325
        },
        "id": "rARDTrrPG9j9",
        "outputId": "961772bd-8497-4ba4-d9ba-70b05b89cb30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The youngest author listed on the \"Attention is All You Need\" paper, which introduced the Transformer architecture, is likely **Aidan N. Gomez**.\n\nHere's why:\n\n* **Aidan N. Gomez** was a PhD student at the University of Oxford at the time of the paper's publication (2017). PhD students are generally earlier in their careers compared to researchers at institutions like Google Brain (where most of the other authors were affiliated).\n\nThe other authors were affiliated with Google Brain and are generally assumed to be more senior researchers:\n\n* Ashish Vaswani\n* Noam Shazeer\n* Niki Parmar\n* Jakob Uszkoreit\n* Llion Jones\n* Łukasz Kaiser\n* Illia Polosukhin\n\nWhile we don't know the exact ages of all the authors at the time of publication, being a PhD student strongly suggests Aidan N. Gomez was the youngest member of the team.\n\n**Therefore, the answer is Aidan N. Gomez.**"
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_contents = !curl https://raw.githubusercontent.com/magicmonty/bash-git-prompt/refs/heads/master/gitprompt.sh\n",
        "\n",
        "explain_prompt = f\"\"\"\n",
        "Please explain what this file does at a very high level. What is it, and why would I use it?\n",
        "\n",
        "```\n",
        "{file_contents}\n",
        "```\n",
        "\"\"\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model='gemini-2.0-flash',\n",
        "    contents=explain_prompt)\n",
        "\n",
        "Markdown(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "id": "kvxBJaaHHeAi",
        "outputId": "f0dd17f9-cec3-448f-8f85-507f06986732"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "This file is a **script designed to enhance your command-line prompt with Git repository information**. It's often referred to as a \"Git prompt\" script.\n\nHere's the breakdown:\n\n*   **What it does:** When sourced (run) in your `.bashrc` or `.zshrc` file, this script modifies your prompt to display details about the Git repository you're currently in. This can include:\n\n    *   The current branch name.\n    *   Whether there are uncommitted changes (staged, unstaged, untracked).\n    *   Whether your branch is ahead or behind the remote branch.\n    *   Information about stashes, conflicts, and other Git states.\n    *   The exit status of the last command.\n    *   Optional virtual environment information.\n\n*   **Why you'd use it:**\n\n    *   **At-a-glance Git status:** It provides a quick visual indicator of the state of your Git repository without having to run `git status` manually.\n    *   **Improved workflow:**  It makes it easier to keep track of your work when using Git, reducing mistakes like committing to the wrong branch or forgetting to stage changes.\n    *   **Customization:** The script is configurable with themes, symbols, and other settings to personalize the prompt to your liking.\n\nIn essence, it's a productivity tool for developers who frequently use Git, providing valuable information directly in the command line prompt.\n"
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "S4UZggf6HthF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}